{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Weight_sharing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWRt02jUjNte"
      },
      "source": [
        "Dataset_size = 10000\n",
        "g_star = 'both'       # low or high or both\n",
        "model = 'WS'     # FC or WS\n",
        "g_known = False   # True or False\n",
        "BATCH_SIZE = 100\n",
        "C = 1\n",
        "Epochs = 500"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gbc2OZhLjNtk",
        "outputId": "9cdcd267-ea69-478b-cb7a-71d1798e4759"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from keras.datasets import mnist\n",
        "import pickle\n",
        "import random\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "class trainData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n",
        "    \n",
        "    \n",
        "class Model_h(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model_h, self).__init__()\n",
        "        \n",
        "        if(model == 'FC') :\n",
        "            self.W = nn.Parameter(torch.tensor(np.zeros((10,10)), dtype = torch.float, requires_grad = True))\n",
        "        elif(model == 'WS') :\n",
        "            self.W = nn.Parameter(torch.tensor(np.zeros((1,10)), dtype = torch.float, requires_grad = True))\n",
        "            \n",
        "        torch.nn.init.xavier_uniform_(self.W)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        \n",
        "        x = inputs * self.W\n",
        "        x = torch.tanh(x)\n",
        "        x = x.sum(dim = -1)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class Model_g(nn.Module) :\n",
        "    def __init__(self, C=1):\n",
        "        super(Model_g, self).__init__()\n",
        "        \n",
        "        self.C = 1\n",
        "\n",
        "        if( not g_known) : \n",
        "            self.layer_list = nn.ModuleList()\n",
        "            self.layer_list.append(nn.Linear(10, 50))\n",
        "            self.layer_list.append(nn.Linear(50, 1))\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        \n",
        "        if(g_known) :\n",
        "            if(g_star=='low') :\n",
        "                return inputs[:, 0]\n",
        "            elif(g_star=='high') :\n",
        "                return torch.cos(inputs[:, :5].sum(-1))\n",
        "            elif(g_star=='both') :\n",
        "                return inputs[:, 0]*self.C+  torch.cos(inputs[:, :5].sum(-1))\n",
        "        else:\n",
        "            x = self.layer_list[0](inputs)\n",
        "            x = torch.relu(x)\n",
        "            x = self.layer_list[1](x)\n",
        "            return x[:,0]\n",
        "\n",
        "#for d_set in [\"gaussian\", \"mnist\"] :\n",
        "for d_set in [ \"mnist\"] :\n",
        "\n",
        "  if(d_set == \"gaussian\") :\n",
        "    temp =[True, False]\n",
        "  else :\n",
        "    temp =[False]\n",
        "\n",
        "  for g_known in temp :\n",
        "\n",
        "    for model in ['WS', 'FC'] :\n",
        "    #for model in ['FC'] :\n",
        "\n",
        "      #for g_star in [\"high\", \"low\", \"both\"] :\n",
        "      for g_star in [\"low\"] :\n",
        "\n",
        "        if (g_star == \"both\") :\n",
        "          #temp2 = [2, 5]\n",
        "          temp2= [0.5]\n",
        "        else :\n",
        "          temp2 = [1]\n",
        "\n",
        "        for C in temp2 :\n",
        "\n",
        "          print(d_set+\"_\" +str(C)+\"_\"+str(g_known)+\"_\"+model+\"_\"+g_star)\n",
        "\n",
        "          np.random.seed(0)\n",
        "\n",
        "          if (d_set == \"gaussian\") :\n",
        "            X = np.random.normal(size = (Dataset_size, 10, 10))\n",
        "          else :\n",
        "            (trainX, trainy), (testX, testy) = mnist.load_data()\n",
        "            trainX = np.expand_dims(trainX, 3)\n",
        "            trainX.shape\n",
        "\n",
        "            X = tf.image.extract_patches(trainX, (1, 10, 10, 1), (1, 5, 5, 1), (1,1,1,1), padding='VALID')\n",
        "            X = np.reshape(X, (-1, 100))\n",
        "            X = np.reshape(X, (X.shape[0], 10, 10))\n",
        "            X = X[np.random.choice(range(X.shape[0]), 10000)]\n",
        "\n",
        "          U_0 = np.random.normal(size = (10,1))\n",
        "\n",
        "          X_temp = np.sum(np.tanh(X * U_0.T), axis = -1)\n",
        "\n",
        "          if(g_star == 'low') :\n",
        "              Y = X_temp[:, 0]\n",
        "          if(g_star == 'high') :\n",
        "              Y = np.cos(np.sum(X_temp[:, :5], axis = -1))\n",
        "          if(g_star == 'both') :\n",
        "              Y = X_temp[:, 0]*C + np.cos(np.sum(X_temp[:, :5], axis = -1))\n",
        "\n",
        "          g = Model_g(C)\n",
        "          h = Model_h()\n",
        "\n",
        "          train_data = trainData(torch.FloatTensor(X), torch.FloatTensor(Y))\n",
        "          train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "          criteria = nn.MSELoss()\n",
        "          optimizer = optim.SGD(list(g.parameters())+list(h.parameters()), lr = 0.01)\n",
        "\n",
        "          g.train()\n",
        "          h.train()\n",
        "\n",
        "          res = np.zeros(Epochs)\n",
        "\n",
        "          for epoch in range(Epochs) :\n",
        "              epoch_loss = 0\n",
        "              for X_batch, y_batch in train_loader :\n",
        "                  optimizer.zero_grad()\n",
        "                  y_pred = g(h(X_batch))\n",
        "                  loss = criteria(y_pred, y_batch)\n",
        "                  loss.backward()\n",
        "                  optimizer.step()\n",
        "                  epoch_loss += loss.item()/2\n",
        "              if(epoch % 50 == 0):\n",
        "                  print(epoch_loss/len(train_loader))\n",
        "              res[epoch] = epoch_loss/len(train_loader)\n",
        "\n",
        "          file = open(d_set+\"_\" +str(C)+\"_\"+str(g_known)+\"_\"+model+\"_\"+g_star, 'wb')\n",
        "          pickle.dump(res, file)\n",
        "          file.close()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mnist_1_False_WS_low\n",
            "0.6188299231231212\n",
            "0.0015229637344600633\n",
            "0.0010277310939272866\n",
            "0.0008231849098228849\n",
            "0.0006666125651099719\n",
            "0.0005177800044475589\n",
            "0.00036486933342530395\n",
            "0.00028293547533394305\n",
            "0.0002517066165819415\n",
            "0.00023797589703463017\n",
            "mnist_1_False_FC_low\n",
            "0.8835350194573403\n",
            "0.24062709808349608\n",
            "0.2024803874641657\n",
            "0.09175483401864767\n",
            "0.08775918759405613\n",
            "0.0849130305275321\n",
            "0.08239796828478575\n",
            "0.08157896568998695\n",
            "0.0798985019698739\n",
            "0.07897066796198488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSdiqq6Uv1-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd01a682-6904-4881-eddf-eca460b88d88"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEBL5Usamcrp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "97de31fb-e852-41c7-a6b6-4b44d0ad8c07"
      },
      "source": [
        "g_star"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'both'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9PQqMMytCf1"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}